{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tK3iwZAjHjLC"
   },
   "source": [
    "# Standalone Example Notebook\n",
    "\n",
    "This notebook is the example notebook but with code copied directly rather than imported.\n",
    "\n",
    "In this standalone notebook we demonstrate CompFS on the Syn1 experiment from the paper. This can be used on custom data if it is written as numpy arrays.\n",
    "\n",
    "This notebook is standalone because we have copied over all code one would need to run CompFS, in some cases it has been slightly adapted, changes do not affect the model, such as changing a function that saves results to simply printing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mpWMfto--idh",
    "outputId": "b4594c75-ae28-4c35-ef13-01949e812781"
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Set and print device.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hbTlRqdgEPVj"
   },
   "source": [
    "# Code Copied Over\n",
    "\n",
    "This code has been copied over and slightly adapted to work in a standalone notebook. The underlying method is exactly the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WGfeJ6UCfeI"
   },
   "source": [
    "## Metrics\n",
    "\n",
    "Accuracy, MSE and GSim metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YMNaI61lF6Ly"
   },
   "outputs": [],
   "source": [
    "def accuracy(x, y):\n",
    "    # Accuracy.\n",
    "    acc = 100 * torch.sum(torch.argmax(x, dim=-1) == y) / len(y)\n",
    "    return acc.item()\n",
    "\n",
    "\n",
    "def mse(x, y):\n",
    "    # MSE for regression.\n",
    "    return 0.5 * torch.mean((x - y) ** 2).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CRcmmFB3CfDo"
   },
   "outputs": [],
   "source": [
    "def gsim(true_groups, predicted_groups):\n",
    "    # Returns gsim, number of true groups, and number of discovered groups, given\n",
    "    # true groups and predicted groups as input.\n",
    "    gsim = 0\n",
    "    if len(true_groups) == 0:  # i.e. we don't know the ground truth.\n",
    "        return -1, len(true_groups), len(predicted_groups)\n",
    "    if len(predicted_groups) > 0:\n",
    "        for g in true_groups:\n",
    "            current_max = 0\n",
    "            for g_hat in predicted_groups:\n",
    "                jac = np.intersect1d(g, g_hat).size / np.union1d(g, g_hat).size\n",
    "                if jac == 1:\n",
    "                    current_max = 1\n",
    "                    break\n",
    "                if jac > current_max:\n",
    "                    current_max = jac\n",
    "            gsim += current_max\n",
    "        gsim /= max(len(true_groups), len(predicted_groups))\n",
    "        return gsim, len(true_groups), len(predicted_groups)\n",
    "    else:  # We didn't find anything.\n",
    "        return 0, len(true_groups), len(predicted_groups)\n",
    "\n",
    "\n",
    "def tpr_fdr(true_groups, predicted_groups):\n",
    "    # True positive rate and false discovery rate.\n",
    "\n",
    "    if len(true_groups) == 0:  # Ground truth not known.\n",
    "        return -1, -1\n",
    "\n",
    "    if len(predicted_groups) == 0:\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    predicted_features = np.unique(reduce(np.union1d, predicted_groups))\n",
    "    true_features = np.unique(reduce(np.union1d, true_groups))\n",
    "\n",
    "    overlap = np.intersect1d(predicted_features, true_features).size\n",
    "    tpr = 100 * overlap / len(true_features)\n",
    "    fdr = (\n",
    "        100 * (len(predicted_features) - overlap) / len(predicted_features)\n",
    "    )  # If len(predicted_features) != 0 else 0.0.\n",
    "    return tpr, fdr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1X5c4HXGBUU"
   },
   "source": [
    "## Thresholding Functions\n",
    "\n",
    "Different thresholding functions for determining feature relevance from feature scores provided by CompFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mb25lBnuGDIj"
   },
   "outputs": [],
   "source": [
    "def make_lambda_threshold(l):\n",
    "    # If the value is above a certain value l (lambda) return 1, otherwise 0.\n",
    "    l = float(l)\n",
    "\n",
    "    def l_func(p):\n",
    "        return p >= torch.full_like(p, l)\n",
    "\n",
    "    return l_func\n",
    "\n",
    "\n",
    "def make_std_threshold(nsigma):\n",
    "    # Choose which features are relevant in p relative to other features,\n",
    "    # if value of feature is above mean + n standard deviations.\n",
    "    nsigma = float(nsigma)\n",
    "\n",
    "    def std_dev_func(p):\n",
    "        mean = torch.mean(p)\n",
    "        std = torch.std(p)\n",
    "        return p >= torch.full_like(p, (mean + nsigma * std).item())\n",
    "\n",
    "    return std_dev_func\n",
    "\n",
    "\n",
    "def make_top_k_threshold(k):\n",
    "    # Choose top k features.\n",
    "    k = int(k)\n",
    "\n",
    "    def top_k(p):\n",
    "        ids = torch.topk(p, k)[1]\n",
    "        out = torch.zeros_like(p)\n",
    "        out[ids] = 1.0\n",
    "        return out.int()\n",
    "\n",
    "    return top_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kpBKeB__edk"
   },
   "source": [
    "## CompFS Model\n",
    "\n",
    "The CompFS torch module and a shell for handling the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o9kpABCx-mc6"
   },
   "outputs": [],
   "source": [
    "def is_array_in_list(arr, arr_list):\n",
    "    \"\"\"Checks if a trial array is in a list of arrays.\"\"\"\n",
    "    for element in arr_list:\n",
    "        if np.array_equal(element, arr):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "class FullyConnected(nn.Module):\n",
    "    \"\"\"\n",
    "    Two hidden layer ReLU MLP, goes to hidden representation ONLY.\n",
    "\n",
    "    Args:\n",
    "        in_dim: the number of features\n",
    "        h_dim: hidden width\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, h_dim):\n",
    "        super(FullyConnected, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, h_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, h_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Gate(nn.Module):\n",
    "    \"\"\"\n",
    "    Gate used in the CompFS individual feature selectors.\n",
    "\n",
    "    Has weights w, and then apply a sigmoid to get p.\n",
    "    When training, sample from Bernoulli with parameters p, using relaxed Bernoulli\n",
    "    to get m.\n",
    "    When testing we apply a thresholding function of choice to p and a step function\n",
    "    to get m.\n",
    "    During training and testing the output of the gate is given by:\n",
    "    gate(x) = m*x + (1-m)*x_bar,\n",
    "    where x_bar is the feature-wise mean of the input.\n",
    "\n",
    "    Args:\n",
    "        in_dim: number of features\n",
    "        threshold_func: function which turns p into m\n",
    "        temp: the \"temperature\"/sharpness of the reparametereised Bernoulli sampling\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, threshold_func, temp):\n",
    "        super(Gate, self).__init__()\n",
    "        self.w = nn.Parameter(torch.normal(torch.zeros(in_dim), torch.ones(in_dim)))\n",
    "        self.threshold_func = threshold_func\n",
    "        self.temp = temp\n",
    "\n",
    "    def forward(self, x, x_bar=0, test=False):\n",
    "        if test:\n",
    "            m = self.make_m()\n",
    "            m = m.repeat(\n",
    "                len(x), 1\n",
    "            ).float()  # Repeat to make it the same size given the batch.\n",
    "        else:\n",
    "            p = torch.sigmoid(self.w).repeat(\n",
    "                len(x), 1\n",
    "            )  # Repeat to make it the same size given the batch.\n",
    "            u = torch.rand(p.shape).to(p.device)\n",
    "            # Reparameterization trick for Bernoulli.\n",
    "            m = torch.sigmoid(\n",
    "                (torch.log(p) - torch.log(1 - p) + torch.log(u) - torch.log(1 - u))\n",
    "                / self.temp\n",
    "            )\n",
    "        return m * x + (1 - m) * x_bar\n",
    "\n",
    "    def make_m(self):\n",
    "        return self.threshold_func(torch.sigmoid(self.w))\n",
    "\n",
    "\n",
    "class SingleFeatureSelector(nn.Module):\n",
    "    \"\"\"\n",
    "    An feature selector based on stochastic gates, given by mlp and Bernoulli gate.\n",
    "    https://arxiv.org/abs/1810.04247\n",
    "\n",
    "    Args:\n",
    "        in_dim: number of features\n",
    "        h_dim: hidden width of learner\n",
    "        out_dim: the dimenion of the output\n",
    "        threshold: threshold for gate\n",
    "        temp: temperature of Bernoulli reparameterisation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, h_dim, out_dim, threshold_func, temp):\n",
    "        super(SingleFeatureSelector, self).__init__()\n",
    "        self.to_hidden = FullyConnected(in_dim, h_dim)\n",
    "        self.gate = Gate(in_dim, threshold_func, temp)\n",
    "        self.fc_individual = nn.Linear(h_dim, out_dim)\n",
    "        self.fc_aggregate = nn.Linear(h_dim, out_dim)\n",
    "\n",
    "    def forward(self, x, x_bar=0, test=False):\n",
    "        return self.to_hidden(self.gate(x, x_bar, test))\n",
    "\n",
    "    def predict(self, x, x_bar):\n",
    "        return self.fc_individual(self.forward(x, x_bar, test=True))\n",
    "\n",
    "    def count_features(self):\n",
    "        # Count how many features there are in this learner.\n",
    "        return torch.sum(self.gate.make_m()).item()\n",
    "\n",
    "    def get_group(self):\n",
    "        # Give the features that this learner uses.\n",
    "        return torch.where(self.gate.make_m())[0]\n",
    "\n",
    "    def get_importance(self):\n",
    "        # Frobenius norm of final weight matrix, to compare to other learners.\n",
    "        return torch.sqrt(torch.sum(self.fc_aggregate.weight**2)).item()\n",
    "\n",
    "\n",
    "class CompFS(nn.Module):\n",
    "    \"\"\"\n",
    "    The CompFS model.\n",
    "\n",
    "    Has a set of weak learners, and given each p vector we punish them overlapping, i.e. p_i dot p_j\n",
    "    and also having lots of features torch.sum(p)**2. We can control how much with beta_s (small groups)\n",
    "    and beta_d (different groups).\n",
    "\n",
    "    Args (in a config_dict):\n",
    "        nlearners: how many groups we want\n",
    "        in_dim: dimension of problem\n",
    "        h_dim: hidden width of mlps\n",
    "        out_dim: dimension of output\n",
    "        threshold: function to determine a feature is included\n",
    "        temp: temperature of the Bernoulli reparameterisation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config_dict):\n",
    "        super(CompFS, self).__init__()\n",
    "        self.beta_s = config_dict[\"beta_s\"]\n",
    "        self.beta_s_decay = config_dict[\"beta_s_decay\"]\n",
    "        self.beta_d = config_dict[\"beta_d\"]\n",
    "        self.beta_d_decay = config_dict[\"beta_d_decay\"]\n",
    "        self.loss_func = config_dict[\"loss_func\"]\n",
    "        self.x_bar = 0\n",
    "        self.nfeatures = config_dict[\"in_dim\"]\n",
    "        self.nlearners = config_dict[\"nlearners\"]\n",
    "        h_dim = config_dict[\"h_dim\"]\n",
    "        out_dim = config_dict[\"out_dim\"]\n",
    "        threshold_func = config_dict[\"threshold_func\"]\n",
    "        temp = config_dict[\"temp\"]\n",
    "        self.learners = nn.ModuleList(\n",
    "            [\n",
    "                SingleFeatureSelector(\n",
    "                    self.nfeatures, h_dim, out_dim, threshold_func, temp\n",
    "                )\n",
    "                for _ in range(self.nlearners)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_b = self.x_bar.repeat(len(x), 1).to(x.device)\n",
    "        total = 0\n",
    "        individuals = torch.tensor([]).to(x.device)\n",
    "        for l in self.learners:\n",
    "            hidden = l(x, x_b).unsqueeze(0)\n",
    "            total += l.fc_aggregate(hidden)\n",
    "            individuals = torch.cat(\n",
    "                [individuals, l.fc_individual(hidden.detach())], dim=0\n",
    "            )\n",
    "        out = torch.cat(\n",
    "            [total, individuals], dim=0\n",
    "        )  # We want to train the ensemble, and the individual learners together.\n",
    "        return out\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Test the ensemble.\n",
    "        x_b = self.x_bar.repeat(len(x), 1).to(x.device)\n",
    "        out = 0\n",
    "        for l in self.learners:\n",
    "            out += l.fc_aggregate(l(x, x_b, test=True))\n",
    "        return out\n",
    "\n",
    "    def preprocess(self, data):\n",
    "        return data\n",
    "\n",
    "    def get_loss(self, x, y):\n",
    "        output = self.forward(x)\n",
    "        loss = self.loss_func(output[0], y)\n",
    "        for i in range(self.nlearners):\n",
    "            loss += self.loss_func(output[i + 1], y)\n",
    "            pi_i = torch.sigmoid(self.learners[i].gate.w)\n",
    "            # Multiply by square root of number of features. So we punish more features, but not as quickly as linearly.\n",
    "            loss += (\n",
    "                self.beta_s\n",
    "                * (torch.mean(pi_i) ** 2)\n",
    "                * (self.nfeatures**0.5)\n",
    "                / (self.nlearners)\n",
    "            )\n",
    "            for j in range(i + 1, self.nlearners):\n",
    "                pi_j = torch.sigmoid(self.learners[j].gate.w)\n",
    "                loss += (\n",
    "                    2\n",
    "                    * self.beta_d\n",
    "                    * torch.mean(pi_i * pi_j)\n",
    "                    * (self.nfeatures**0.5)\n",
    "                    / (self.nlearners * (self.nlearners - 1))\n",
    "                )\n",
    "        return loss\n",
    "\n",
    "    def update_after_epoch(self):\n",
    "        self.beta_d *= self.beta_d_decay\n",
    "        self.beta_s *= self.beta_s_decay\n",
    "\n",
    "    def count_features(self):\n",
    "        # Return list of number of features in each group.\n",
    "        out = []\n",
    "        for l in self.learners:\n",
    "            out.append(l.count_features())\n",
    "        return out\n",
    "\n",
    "    def get_overlap(self):\n",
    "        # Count how many features overlap, and where they are.\n",
    "        overlap = 0\n",
    "        for l in self.learners:\n",
    "            overlap += l.gate.make_m()\n",
    "        overlap = overlap > 1\n",
    "        noverlap = torch.sum(overlap).item()\n",
    "        ids = torch.where(overlap)\n",
    "        return noverlap, ids\n",
    "\n",
    "    def get_groups(self):\n",
    "        # Return a list of the groups as numpy arrays, which are not empty and unique.\n",
    "        groups = []\n",
    "        for l in self.learners:\n",
    "            g = l.get_group().detach().cpu().numpy()\n",
    "            if (len(g) != 0) and (not is_array_in_list(g, groups)):\n",
    "                groups.append(g)\n",
    "        return groups\n",
    "\n",
    "    def set_threshold_func(self, new_func):\n",
    "        # After training we can change how we threshold the scores of each learner. By giving\n",
    "        # the ensemble a new thresholding function.\n",
    "        for l in self.learners:\n",
    "            l.gate.threshold_func = new_func\n",
    "\n",
    "    def print_evaluation_info(self, x, y, val_metric):\n",
    "        output = self.predict(x)\n",
    "        full_model_performance = val_metric(output, y)\n",
    "        print(\n",
    "            \"\\n\\nPerformance:\\nFull Model Test Metric: {:.3f}\".format(\n",
    "                full_model_performance\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # print individual accuracies if using compfs\n",
    "        for i in range(self.nlearners):\n",
    "            output = self.learners[i].predict(\n",
    "                x, self.x_bar.repeat(len(x), 1).to(x.device)\n",
    "            )\n",
    "            individual_performance = val_metric(output, y)\n",
    "            print(\n",
    "                \"Group: {}, Test Metric: {:.3f}\".format(i + 1, individual_performance)\n",
    "            )\n",
    "\n",
    "        # print importances if using compfs\n",
    "        print(\"\\n\\nImportances:\")\n",
    "        for i in range(self.nlearners):\n",
    "            individual_importance = self.learners[i].get_importance()\n",
    "            print(\"Group: {}, Importance: {:.3f}\".format(i + 1, individual_importance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4DyLnYCh-95w"
   },
   "outputs": [],
   "source": [
    "class CompFSShell:\n",
    "    def __init__(self, model_config):\n",
    "        self.device = device\n",
    "        self.model = CompFS(model_config).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=model_config[\"lr\"])\n",
    "        self.num_epochs = model_config[\"num_epochs\"]\n",
    "        self.lr_decay = model_config[\"lr_decay\"]\n",
    "        self.batchsize = model_config[\"batchsize\"]\n",
    "        self.val_metric = model_config[\"val_metric\"]\n",
    "        super().__init__()\n",
    "\n",
    "    def train(self, train_data, val_data):\n",
    "        self.model.x_bar = train_data.get_x_bar()\n",
    "        train_data = self.model.preprocess(train_data)\n",
    "        val_data = self.model.preprocess(val_data)\n",
    "        batch_size = len(train_data) if self.batchsize == 0 else self.batchsize\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_data, batch_size=len(val_data), shuffle=False)\n",
    "        print(\"\\n\\nTraining for {} Epochs:\\n\".format(self.num_epochs))\n",
    "\n",
    "        for epoch in range(1, self.num_epochs + 1):\n",
    "            # Train an epoch.\n",
    "            epoch_loss = self.train_epoch(train_loader)\n",
    "\n",
    "            # Evaluate the model and save values.\n",
    "            val = self.calculate_val_metric(val_loader)\n",
    "            nfeatures = self.model.count_features()\n",
    "            overlap = self.model.get_overlap()[0]\n",
    "\n",
    "            # Print information.\n",
    "            print(\n",
    "                \"Epoch: {}, Average Loss: {:.3f}, Val Metric: {:.1f}, nfeatures: {}, Overlap: {}\".format(\n",
    "                    epoch, epoch_loss, val, nfeatures, overlap\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Update learning rate.\n",
    "            for g in self.optimizer.param_groups:\n",
    "                g[\"lr\"] *= self.lr_decay\n",
    "\n",
    "    def train_epoch(self, train_loader):\n",
    "        avg_loss = 0\n",
    "        for x, y in train_loader:\n",
    "            x = x.view(x.shape[0], -1)  # Flatten to vectors.\n",
    "            x = x.to(self.device)\n",
    "            y = y.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.model.get_loss(x, y)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            avg_loss += loss.item()\n",
    "        self.model.update_after_epoch()\n",
    "        return avg_loss / len(train_loader)\n",
    "\n",
    "    def calculate_val_metric(self, val_loader):\n",
    "        metric = 0\n",
    "        for x, y in val_loader:\n",
    "            x = x.view(x.shape[0], -1)  # Flatten to vectors.\n",
    "            x = x.to(self.device)\n",
    "            y = y.to(self.device)\n",
    "            out = self.model.predict(x)\n",
    "            metric += self.val_metric(out, y)\n",
    "        return metric / len(val_loader)\n",
    "\n",
    "    def get_groups(self):\n",
    "        return self.model.get_groups()\n",
    "\n",
    "    def print_evaluation_info(self, val_data):\n",
    "        val_loader = DataLoader(val_data, batch_size=len(val_data), shuffle=False)\n",
    "        for x, y in val_loader:\n",
    "            x = x.view(x.shape[0], -1)  # flatten the vectors\n",
    "            x = x.to(self.device)\n",
    "            y = y.to(self.device)\n",
    "        self.model.print_evaluation_info(x, y, self.val_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ptCAoykC8FC"
   },
   "source": [
    "## Dataset Class\n",
    "\n",
    "This class takes NumPy X and y data and converts it into a dataset that works with CompFS. This can be used on your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VCZbh05cDGx6"
   },
   "outputs": [],
   "source": [
    "class NumpyDataSet(Dataset):\n",
    "    def __init__(self, X_data, y_data, classification=True):\n",
    "\n",
    "        self.x_bar = torch.tensor(np.mean(X_data, axis=0)).float()\n",
    "        self.num_data = X_data.shape[0]\n",
    "        self.data = []\n",
    "        for x_sample, y_sample in zip(X_data, y_data):\n",
    "            x = torch.from_numpy(x_sample).float()\n",
    "            if classification:\n",
    "                y = torch.tensor(y_sample).long()\n",
    "            else:\n",
    "                y = torch.tensor(y_sample).float()\n",
    "            self.data.append((x, y))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_data\n",
    "\n",
    "    def get_x_bar(self):\n",
    "        try:\n",
    "            return self.x_bar\n",
    "        except AttributeError:\n",
    "            x_bar = 0\n",
    "            for sample in self.data:\n",
    "                x_bar += sample[0]\n",
    "            self.x_bar = x_bar / self.num_data\n",
    "            return self.x_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKlg239SDEPs"
   },
   "source": [
    "# Example\n",
    "\n",
    "Here we demonstrate CompFS on Syn1. The two cells below can be edited to run your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "afsRGOZjEKMQ"
   },
   "outputs": [],
   "source": [
    "# These can be changed to run your own data.\n",
    "\n",
    "X_train = np.random.normal(size=(20000, 500))\n",
    "y_train = np.array([((x[0] > 0.55) or (x[1] > 0.55)) for x in X_train])\n",
    "X_val = np.random.normal(size=(200, 500))\n",
    "y_val = np.array([((x[0] > 0.55) or (x[1] > 0.55)) for x in X_val])\n",
    "\n",
    "classification = True\n",
    "\n",
    "ground_truth_groups = [np.array([0]), np.array([1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGlzWZtLFhs6"
   },
   "outputs": [],
   "source": [
    "# This config should be changed to use your own data, and find specific\n",
    "# hyperparameters for the problem.\n",
    "\n",
    "compfs_config = {\n",
    "    \"lr\": 0.003,\n",
    "    \"lr_decay\": 0.99,\n",
    "    \"batchsize\": 50,\n",
    "    \"num_epochs\": 10,\n",
    "    \"loss_func\": nn.CrossEntropyLoss(),\n",
    "    \"val_metric\": accuracy,\n",
    "    \"in_dim\": 500,\n",
    "    \"h_dim\": 20,\n",
    "    \"out_dim\": 2,\n",
    "    \"nlearners\": 5,\n",
    "    \"threshold_func\": make_lambda_threshold(0.7),\n",
    "    \"temp\": 0.1,\n",
    "    \"beta_s\": 4.5,\n",
    "    \"beta_s_decay\": 0.99,\n",
    "    \"beta_d\": 1.2,\n",
    "    \"beta_d_decay\": 0.99,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lcG_8oYLE7FI"
   },
   "outputs": [],
   "source": [
    "train_data = NumpyDataSet(X_train, y_train, classification=classification)\n",
    "val_data = NumpyDataSet(X_val, y_val, classification=classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lXzw00vqFFG_"
   },
   "outputs": [],
   "source": [
    "model = CompFSShell(compfs_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "laOgQJDPGMNT",
    "outputId": "3454f841-e9b7-47c1-c420-ca262580e28c"
   },
   "outputs": [],
   "source": [
    "model.train(train_data, val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p5OASitsGcQ9",
    "outputId": "3d723bd0-91b7-4d77-f817-b98fb0c49d67"
   },
   "outputs": [],
   "source": [
    "model.print_evaluation_info(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9tiWX3lWGXcf",
    "outputId": "ce83a0d1-c833-4858-84ed-844db72df098"
   },
   "outputs": [],
   "source": [
    "# Get group similarity and group structure.\n",
    "tpr, fdr = tpr_fdr(ground_truth_groups, model.get_groups())\n",
    "group_sim, ntrue, npredicted = gsim(ground_truth_groups, model.get_groups())\n",
    "\n",
    "print(\"\\n\\nGroup Structure:\")\n",
    "print(\n",
    "    \"Group Similarity: {:.3f}, True Positive Rate: {:.3f}%, False Discovery Rate: {:.3f}%\".format(\n",
    "        group_sim, tpr, fdr\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Number of True Groups: {}, Number of Predicted Groups: {}\".format(\n",
    "        ntrue, npredicted\n",
    "    )\n",
    ")\n",
    "\n",
    "# Give selected features and save the groups.\n",
    "print(\"\\n\\nSelected Features:\")\n",
    "learnt_groups = model.get_groups()\n",
    "for i in range(len(learnt_groups)):\n",
    "    print(\"Group: {}, Features: {}\".format(i + 1, learnt_groups[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wsn30ZLfIIgF"
   },
   "source": [
    "We see that the model finds the features, usually separating features 0 and 1, occasionally grouping them together."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
