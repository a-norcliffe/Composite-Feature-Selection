{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CAE Notebook\n",
        "\n",
        "To use this notebook, put the appropriate Chemistry Data in a folder in Google Drive, after mounting the drive it is crucial to select the correct folder, which is the first line of this notebook:\n",
        "\n",
        "foldername = 'gdrive/MyDrive/'\n",
        "\n",
        "Edit this to have the correct foldername.\n",
        "\n",
        "We also need to install STG, with pip install, then restart the runtime.\n",
        "\n",
        "After that it is fairly easy to run. Hyperparameters can be changed, and in the Run Experiment part of the notebook one can run a synthetic or chemistry experiment easily by commenting out which experiment to run later."
      ],
      "metadata": {
        "id": "O4BnVsTvGYBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "foldername = 'gdrive/MyDrive/"
      ],
      "metadata": {
        "id": "X02UZBGtGyXj"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SWQQWNMXC05A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51923b7f-fe0b-4d09-d6ce-9c0ba0e14581"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stg\n",
            "  Downloading stg-0.1.2.tar.gz (14 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from stg) (1.12.1+cu113)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from stg) (3.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from stg) (1.15.0)\n",
            "Collecting lifelines\n",
            "  Downloading lifelines-0.27.3-py3-none-any.whl (349 kB)\n",
            "\u001b[K     |████████████████████████████████| 349 kB 7.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from h5py->stg) (1.21.6)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->stg) (1.5.2)\n",
            "Collecting autograd-gamma>=0.3\n",
            "  Downloading autograd-gamma-0.5.0.tar.gz (4.0 kB)\n",
            "Requirement already satisfied: autograd>=1.3 in /usr/local/lib/python3.7/dist-packages (from lifelines->stg) (1.5)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.7/dist-packages (from lifelines->stg) (3.2.2)\n",
            "Collecting formulaic>=0.2.2\n",
            "  Downloading formulaic-0.5.2-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 7.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from lifelines->stg) (1.7.3)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from lifelines->stg) (1.3.5)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.7/dist-packages (from autograd>=1.3->lifelines->stg) (0.16.0)\n",
            "Collecting graphlib-backport>=1.0.0\n",
            "  Downloading graphlib_backport-1.0.3-py3-none-any.whl (5.1 kB)\n",
            "Collecting typing-extensions>=4.2.0\n",
            "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: astor>=0.8 in /usr/local/lib/python3.7/dist-packages (from formulaic>=0.2.2->lifelines->stg) (0.8.1)\n",
            "Collecting interface-meta>=1.2.0\n",
            "  Downloading interface_meta-1.3.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.7/dist-packages (from formulaic>=0.2.2->lifelines->stg) (1.14.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines->stg) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines->stg) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines->stg) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines->stg) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.0->lifelines->stg) (2022.5)\n",
            "Building wheels for collected packages: stg, autograd-gamma\n",
            "  Building wheel for stg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stg: filename=stg-0.1.2-py3-none-any.whl size=15522 sha256=dba8a4ce150fc38d988ebd9aba748a018a84b41f5f29611f5eda078a98d9e13c\n",
            "  Stored in directory: /root/.cache/pip/wheels/44/31/41/0ccce37fd51a6ca0672669d25dfc154400dcf9b4748895d4c7\n",
            "  Building wheel for autograd-gamma (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autograd-gamma: filename=autograd_gamma-0.5.0-py3-none-any.whl size=4048 sha256=6ff3e8ce369a83050f4d2b87adee708364445f49ca95bb72438c035931be1274\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/01/ee/1331593abb5725ff7d8c1333aee93a50a1c29d6ddda9665c9f\n",
            "Successfully built stg autograd-gamma\n",
            "Installing collected packages: typing-extensions, interface-meta, graphlib-backport, formulaic, autograd-gamma, lifelines, stg\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.1.5 requires typing-extensions<4.2.0,>=3.7.4.1; python_version < \"3.8\", but you have typing-extensions 4.4.0 which is incompatible.\n",
            "spacy 3.4.2 requires typing-extensions<4.2.0,>=3.7.4; python_version < \"3.8\", but you have typing-extensions 4.4.0 which is incompatible.\n",
            "confection 0.0.3 requires typing-extensions<4.2.0,>=3.7.4.1; python_version < \"3.8\", but you have typing-extensions 4.4.0 which is incompatible.\u001b[0m\n",
            "Successfully installed autograd-gamma-0.5.0 formulaic-0.5.2 graphlib-backport-1.0.3 interface-meta-1.3.0 lifelines-0.27.3 stg-0.1.2 typing-extensions-4.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --user stg \n",
        "# If you are running this notebook on Google Colab, please reset the current python environment via 'Runtime -> Restart runtime' after installation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "BBIk7eYZC05J"
      },
      "outputs": [],
      "source": [
        "from stg import STG\n",
        "import numpy as np\n",
        "import scipy.stats # for creating a simple dataset \n",
        "import matplotlib.pyplot as plt \n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from functools import reduce\n",
        "\n",
        "import os\n",
        "import os.path as osp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(x):\n",
        "    # set a consistent seed, so we can run across different runs\n",
        "    x *= 10000\n",
        "    np.random.seed(x)\n",
        "    torch.manual_seed(x)\n",
        "    torch.cuda.manual_seed(x)\n",
        "    torch.cuda.manual_seed_all(x)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n"
      ],
      "metadata": {
        "id": "ZeMArOElRnPO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Syn Data Specific"
      ],
      "metadata": {
        "id": "07RcRDF3RGx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logic Rules.\n",
        "\n",
        "def rule1(x_sample):\n",
        "    return (x_sample[0] > 0.55) or (x_sample[1] > 0.55)\n",
        "\n",
        "def rule2(x_sample):\n",
        "    return (x_sample[0]*x_sample[1] > 0.30) or (x_sample[2]*x_sample[3] > 0.30)\n",
        "\n",
        "def rule3(x_sample):\n",
        "    return (x_sample[0]*x_sample[1] > 0.30) or (x_sample[0]*x_sample[2] > 0.30)\n",
        "\n",
        "def rule4(x_sample):\n",
        "    return (x_sample[0]*x_sample[3] > 0.30) or (x_sample[6]*x_sample[9] > 0.30)\n",
        "\n",
        "# Sampling rules\n",
        "\n",
        "def normal_sample(nsamples, nfeatures):\n",
        "    return np.random.normal(size=(nsamples, nfeatures))\n",
        "\n",
        "def correlated_sample(nsamples, nfeatures):\n",
        "    mean = np.array([0.0, 0.0, 0.0])\n",
        "    cov = np.array([[1, 0.99, 0.99],\n",
        "                    [0.99, 1, 0.99],\n",
        "                    [0.99, 0.99, 1]])\n",
        "\n",
        "    x123 = np.random.multivariate_normal(mean, cov, size=nsamples)\n",
        "    x456 = np.random.multivariate_normal(mean, cov, size=nsamples)\n",
        "    x789 = np.random.multivariate_normal(mean, cov, size=nsamples)\n",
        "    x101112 = np.random.multivariate_normal(mean, cov, size=nsamples)\n",
        "    xrest = np.random.normal(size=(nsamples, nfeatures-4*3))\n",
        "    return np.concatenate([x123, x456, x789, x101112, xrest], axis=1)\n",
        "\n",
        "sampling_rules = {\n",
        "        1: normal_sample,\n",
        "        2: normal_sample,\n",
        "        3: normal_sample,\n",
        "        4: correlated_sample\n",
        "    }\n",
        "\n",
        "logic_rules = {\n",
        "    1: rule1,\n",
        "    2: rule2,\n",
        "    3: rule3,\n",
        "    4: rule4\n",
        "}\n",
        "\n",
        "gauss_groups = {1: [np.array([0]), np.array([1])], 2: [np.array([0, 1]), np.array([2, 3])],\n",
        "                3: [np.array([0, 1]), np.array([0, 2])], 4: [np.array([0, 3]), np.array([6, 9])]}\n",
        "gauss_oracle_features = {1: np.array([0, 1]), 2: np.array([0, 1, 2, 3]),\n",
        "                         3: np.array([0, 1, 2]), 4: np.array([0, 3, 6, 9])}\n",
        "\n",
        "\n",
        "\n",
        "def make_syn_data(rule, nsamples, nfeatures, train):\n",
        "    x_data = sampling_rules[rule](nsamples, nfeatures)\n",
        "    y_data = np.array([logic_rules[rule](x) for x in x_data])\n",
        "    return x_data, y_data"
      ],
      "metadata": {
        "id": "5n2x77I0RKU0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chem Data Specific"
      ],
      "metadata": {
        "id": "yRoZkVKoREnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('gdrive')\n",
        "\n",
        "\n",
        "chem_data_groups = {4: [np.array([40]), np.array([1])], # logic_4 = ether OR NOT alkyne\n",
        "\t\t            10: [np.array([56, 18]), np.array([40])], # logic_10 = (primary amine AND NOT ether) OR (NOT benzene AND NOT ether)\n",
        "\t\t            13: [np.array([18, 29]), np.array([1, 40])], # logic_13 = (benzene AND NOT carbonyl) OR (alkyne AND NOT ether)\n",
        "                    }\n",
        "chem_oracle_features = {4: np.array([1, 40]),\n",
        "                        10: np.array([18, 40, 56]),\n",
        "                        13: np.array([1, 18, 29, 40])}\n",
        "\n",
        "\n",
        "def make_chem_data(rule, train=True):\n",
        "    is_train = 'train' if train else 'test'\n",
        "    x_data = np.load(osp.join(foldername, 'logic_'+str(rule)+'_X_'+is_train+'.npy'))\n",
        "    y_data = np.load(osp.join(foldername, 'logic_'+str(rule)+'_Y_'+is_train+'.npy'))\n",
        "    return x_data, y_data\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "blaIQ6xCmFCm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01227d35-f1ac-49d7-fdb6-260f069b8340"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics"
      ],
      "metadata": {
        "id": "IQEB8BSgOXPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gsim(true_groups, predicted_groups):\n",
        "    # Returns gsim, number of true groups, and number of discovered groups, given\n",
        "    # true groups and predicted groups as input.\n",
        "    gsim = 0\n",
        "    if len(true_groups) == 0: # i.e. we don't know the ground truth.\n",
        "       return -1, len(true_groups), len(predicted_groups)\n",
        "    if len(predicted_groups)>0:\n",
        "      for g in true_groups:\n",
        "         current_max = 0\n",
        "         for g_hat in predicted_groups:\n",
        "            jac = np.intersect1d(g, g_hat).size / np.union1d(g, g_hat).size\n",
        "            if jac == 1:\n",
        "               current_max = 1\n",
        "               break\n",
        "            if jac > current_max:\n",
        "               current_max = jac\n",
        "         gsim += current_max\n",
        "      gsim /= max(len(true_groups), len(predicted_groups))\n",
        "      return gsim, len(true_groups), len(predicted_groups)\n",
        "    else:   # We didn't find anything.\n",
        "      return 0, len(true_groups), len(predicted_groups)\n",
        "\n",
        "\n",
        "def tpr_fdr(true_groups, predicted_groups):\n",
        "   # True positive rate and false discovery rate.\n",
        "   \n",
        "   if len(true_groups) == 0:  # Ground truth not known.\n",
        "      return -1, -1\n",
        "\n",
        "   if len(predicted_groups) == 0:\n",
        "      return 0.0, 0.0\n",
        "\n",
        "   predicted_features = np.unique(reduce(np.union1d, predicted_groups))\n",
        "   true_features = np.unique(reduce(np.union1d, true_groups))\n",
        "\n",
        "   overlap = np.intersect1d(predicted_features, true_features).size\n",
        "   tpr = 100*overlap/len(true_features)\n",
        "   fdr = 100*(len(predicted_features)-overlap)/len(predicted_features) # If len(predicted_features) != 0 else 0.0.\n",
        "   return tpr, fdr"
      ],
      "metadata": {
        "id": "1wnEo47U1Ref"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Data\n",
        "\n",
        "Edit the first cell below to choose which data to use."
      ],
      "metadata": {
        "id": "24jzGV3oQWoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Edit these lines, choice of experiment is syn or chem\n",
        "\n",
        "choice = 'syn'     # Uncomment one of these \n",
        "#choice = 'chem'     # Uncomment one of these\n",
        "rule = 2\n",
        "experiment_no = 1"
      ],
      "metadata": {
        "id": "T0icrO2-RjHA"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(experiment_no)\n",
        "\n",
        "if choice == 'syn':\n",
        "    train_size = 20000\n",
        "    test_size = 200\n",
        "    nfeatures = 500\n",
        "    batchsize = 500\n",
        "    lr = 0.001\n",
        "    nepochs = 400\n",
        "    lam = 0.1\n",
        "\n",
        "    X_train, y_train = make_syn_data(rule, train_size, nfeatures, train=True)\n",
        "    X_test, y_test = make_syn_data(rule, test_size, nfeatures, train=False)\n",
        "    true_groups = gauss_groups[rule]\n",
        "\n",
        "if choice == 'chem':\n",
        "    batchsize = 200\n",
        "    lr = 0.001\n",
        "    nepochs = 400\n",
        "    lam = 0.1\n",
        "\n",
        "    rules = {\n",
        "        1: 4,\n",
        "        2: 10,\n",
        "        3: 13\n",
        "    }\n",
        "    rule = rules[rule]\n",
        "    \n",
        "\n",
        "    X_train, y_train = make_chem_data(rule, train=True)\n",
        "    X_test, y_test = make_chem_data(rule, train=False)\n",
        "    nfeatures = X_train.shape[-1]\n",
        "    true_groups = chem_data_groups[rule]\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjnJlGPqmKp_",
        "outputId": "3fb30119-c8f5-4929-c4b1-85895fa722bb"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20000, 500)\n",
            "(20000,)\n",
            "(200, 500)\n",
            "(200,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train STG"
      ],
      "metadata": {
        "id": "-J0tcf2XFRXw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "g1lNcd7HC05O"
      },
      "outputs": [],
      "source": [
        "args_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if args_cuda else \"cpu\") \n",
        "feature_selection = True\n",
        "model = STG(task_type='classification',input_dim=X_train.shape[1], output_dim=2, hidden_dims=[20, 20], activation='relu',\n",
        "    optimizer='Adam', learning_rate=lr, batch_size=batchsize, feature_selection=feature_selection, sigma=0.5, lam=lam, device=device) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "tags": [],
        "id": "6ihiuNI-C05P",
        "outputId": "10aad7d3-f548-4d35-db0d-4b54b56e190a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 25: loss=0.390269 valid_loss=0.227261\n",
            "Epoch: 50: loss=0.259313 valid_loss=0.163725\n",
            "Epoch: 75: loss=0.196950 valid_loss=0.133095\n",
            "Epoch: 100: loss=0.145382 valid_loss=0.117209\n",
            "Epoch: 125: loss=0.132095 valid_loss=0.090211\n",
            "Epoch: 150: loss=0.133133 valid_loss=0.096924\n",
            "Epoch: 175: loss=0.122266 valid_loss=0.081826\n",
            "Epoch: 200: loss=0.114965 valid_loss=0.080297\n",
            "Epoch: 225: loss=0.115604 valid_loss=0.068075\n",
            "Epoch: 250: loss=0.128641 valid_loss=0.083237\n",
            "Epoch: 275: loss=0.112951 valid_loss=0.072917\n",
            "Epoch: 300: loss=0.115203 valid_loss=0.065719\n",
            "Epoch: 325: loss=0.110870 valid_loss=0.074739\n",
            "Epoch: 350: loss=0.109901 valid_loss=0.068278\n",
            "Epoch: 375: loss=0.153731 valid_loss=0.064726\n",
            "Epoch: 400: loss=0.109299 valid_loss=0.060470\n"
          ]
        }
      ],
      "source": [
        "model.fit(X_train, y_train, nr_epochs=nepochs, valid_X=X_test, valid_y=y_test, print_interval=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_F8MHMhC05W"
      },
      "source": [
        "## Testing STG"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selected = np.where(model.get_gates(mode='prob').astype(int))[0]\n",
        "y_pred=model.predict(X_test)\n",
        "acc = 100*np.mean(y_pred==y_test)\n",
        "tpr, fdr = tpr_fdr(true_groups, [selected])\n",
        "group_similarity, num_true_groups, num_discovered_groups = gsim(true_groups, [selected])\n",
        "\n",
        "\n",
        "print('Stg Performance:')\n",
        "print('Accuracy: {:.3f}%'.format(acc))\n",
        "print('Selected features: {}'.format(selected))\n",
        "print('TPR: {:.3f}%'.format(tpr))\n",
        "print('FDR: {:.3f}%'.format(fdr))\n",
        "print('Gsim: {:.3f}'.format(group_similarity))\n",
        "print('Num True Groups: {}'.format(num_true_groups))\n",
        "print('Num Discovered Groups: {}'.format(num_discovered_groups))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PO8IEdHKORB",
        "outputId": "db752a04-1ae7-4630-b1bb-61b6b1fb1edf"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stg Performance:\n",
            "Accuracy: 97.500%\n",
            "Selected features: [0 1 2 3]\n",
            "TPR: 100.000%\n",
            "FDR: 0.000%\n",
            "Gsim: 0.500\n",
            "Num True Groups: 2\n",
            "Num Discovered Groups: 1\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "version": "3.7.2-final"
    },
    "orig_nbformat": 2,
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "kernelspec": {
      "name": "python37264bitstgvenvb804c3810c64404c8da0a62ab054ff5f",
      "display_name": "Python 3.7.2 64-bit ('stg': venv)"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "07RcRDF3RGx-",
        "yRoZkVKoREnG",
        "IQEB8BSgOXPf"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
