{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tK3iwZAjHjLC"
   },
   "source": [
    "# Example Notebook\n",
    "\n",
    "In this notebook we demonstrate CompFS on the Syn1 experiment from the paper. This can be used on custom data if it is written as numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mpWMfto--idh",
    "outputId": "b4594c75-ae28-4c35-ef13-01949e812781"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Set and print device.\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required files from CompFS Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexn\\anaconda3\\envs\\compfsenv\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from model.metrics import accuracy, mse, gsim, tpr_fdr\n",
    "from model.thresholding_functions import make_lambda_threshold, make_std_threshold, make_top_k_threshold\n",
    "from model.compfs import CompFS\n",
    "from model.base_model import TorchModel\n",
    "from datasets.datasets import NumpyDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKlg239SDEPs"
   },
   "source": [
    "# Example\n",
    "\n",
    "Here we demonstrate CompFS on Syn1. The two cells below can be edited to run your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "afsRGOZjEKMQ"
   },
   "outputs": [],
   "source": [
    "# These can be changed to run your own data.\n",
    "\n",
    "X_train = np.random.normal(size=(20000, 500))\n",
    "y_train = np.array([((x[0] > 0.55) or (x[1] > 0.55)) for x in X_train])\n",
    "X_val = np.random.normal(size=(200, 500))\n",
    "y_val = np.array([((x[0] > 0.55) or (x[1] > 0.55)) for x in X_val])\n",
    "\n",
    "is_classification = True\n",
    "\n",
    "ground_truth_groups = [np.array([0]), np.array([1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jGlzWZtLFhs6"
   },
   "outputs": [],
   "source": [
    "# This config should be changed to use your own data, and find specific\n",
    "# hyperparameters for the problem.\n",
    "\n",
    "compfs_config = {\n",
    "    'model': CompFS,\n",
    "    'model_config': {\n",
    "        'lr': 0.003,\n",
    "        'lr_decay': 0.99,\n",
    "        'batchsize': 50,\n",
    "        'num_epochs': 35,\n",
    "        'loss_func': nn.CrossEntropyLoss(),\n",
    "        'val_metric': accuracy,\n",
    "        'in_dim': 500,\n",
    "        'h_dim': 20,\n",
    "        'out_dim': 2,\n",
    "        'nlearners': 5,\n",
    "        'threshold_func': make_lambda_threshold(0.7),\n",
    "        'temp': 0.1,\n",
    "        'beta_s': 4.5,\n",
    "        'beta_s_decay': 0.99,\n",
    "        'beta_d': 1.2,\n",
    "        'beta_d_decay': 0.99   \n",
    "    }   \n",
    "}\n",
    "\n",
    "compfs_config['device'] = device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a CompFS Model and see the Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lcG_8oYLE7FI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training for 35 Epochs:\n",
      "\n",
      "Epoch: 1, Average Loss: 22.872, Val Metric: 91.5, nfeatures: [11, 10, 10, 10, 14], Overlap: 3\n",
      "Epoch: 2, Average Loss: 9.827, Val Metric: 92.0, nfeatures: [2, 1, 0, 1, 1], Overlap: 2\n",
      "Epoch: 3, Average Loss: 5.728, Val Metric: 91.5, nfeatures: [1, 1, 0, 1, 1], Overlap: 2\n",
      "Epoch: 4, Average Loss: 4.283, Val Metric: 95.0, nfeatures: [1, 1, 0, 1, 1], Overlap: 2\n",
      "Epoch: 5, Average Loss: 3.656, Val Metric: 96.0, nfeatures: [1, 1, 0, 1, 1], Overlap: 2\n",
      "Epoch: 6, Average Loss: 3.306, Val Metric: 96.0, nfeatures: [1, 1, 0, 1, 1], Overlap: 2\n",
      "Epoch: 7, Average Loss: 3.109, Val Metric: 97.5, nfeatures: [1, 1, 0, 1, 1], Overlap: 2\n",
      "Epoch: 8, Average Loss: 2.976, Val Metric: 96.5, nfeatures: [1, 1, 0, 1, 1], Overlap: 2\n",
      "Epoch: 9, Average Loss: 2.883, Val Metric: 98.5, nfeatures: [1, 1, 0, 1, 1], Overlap: 2\n",
      "Epoch: 10, Average Loss: 2.828, Val Metric: 98.5, nfeatures: [1, 1, 0, 1, 1], Overlap: 2\n",
      "Epoch: 11, Average Loss: 2.773, Val Metric: 98.0, nfeatures: [1, 1, 0, 1, 1], Overlap: 2\n",
      "Epoch: 12, Average Loss: 2.730, Val Metric: 98.5, nfeatures: [1, 1, 0, 1, 1], Overlap: 2\n",
      "Epoch: 13, Average Loss: 2.711, Val Metric: 98.5, nfeatures: [1, 1, 0, 1, 1], Overlap: 2\n",
      "Epoch: 14, Average Loss: 2.680, Val Metric: 99.5, nfeatures: [1, 1, 0, 1, 1], Overlap: 2\n",
      "Epoch: 15, Average Loss: 2.667, Val Metric: 100.0, nfeatures: [1, 1, 0, 1, 1], Overlap: 2\n",
      "Epoch: 16, Average Loss: 2.649, Val Metric: 98.5, nfeatures: [1, 1, 0, 1, 1], Overlap: 2\n",
      "Epoch: 17, Average Loss: 2.638, Val Metric: 100.0, nfeatures: [1, 1, 0, 1, 1], Overlap: 2\n",
      "Epoch: 18, Average Loss: 2.627, Val Metric: 99.0, nfeatures: [1, 1, 0, 1, 1], Overlap: 2\n",
      "Epoch: 19, Average Loss: 2.616, Val Metric: 100.0, nfeatures: [1, 1, 0, 1, 1], Overlap: 2\n",
      "Epoch: 20, Average Loss: 2.603, Val Metric: 100.0, nfeatures: [1, 1, 0, 1, 1], Overlap: 2\n",
      "Epoch: 21, Average Loss: 2.594, Val Metric: 100.0, nfeatures: [1, 1, 0, 1, 1], Overlap: 2\n",
      "Epoch: 22, Average Loss: 2.594, Val Metric: 99.0, nfeatures: [1, 1, 0, 1, 1], Overlap: 2\n",
      "Epoch: 23, Average Loss: 2.585, Val Metric: 100.0, nfeatures: [1, 1, 0, 1, 1], Overlap: 2\n",
      "Epoch: 24, Average Loss: 2.583, Val Metric: 99.5, nfeatures: [1, 1, 0, 1, 1], Overlap: 2\n",
      "Epoch: 25, Average Loss: 2.578, Val Metric: 100.0, nfeatures: [1, 1, 0, 1, 1], Overlap: 2\n",
      "Epoch: 26, Average Loss: 2.564, Val Metric: 99.5, nfeatures: [1, 1, 0, 1, 1], Overlap: 2\n",
      "Epoch: 27, Average Loss: 2.567, Val Metric: 100.0, nfeatures: [1, 1, 0, 1, 1], Overlap: 2\n",
      "Epoch: 28, Average Loss: 2.557, Val Metric: 100.0, nfeatures: [1, 1, 0, 1, 1], Overlap: 2\n",
      "Epoch: 29, Average Loss: 2.556, Val Metric: 100.0, nfeatures: [1, 1, 0, 1, 1], Overlap: 2\n",
      "Epoch: 30, Average Loss: 2.553, Val Metric: 100.0, nfeatures: [1, 1, 0, 1, 1], Overlap: 2\n",
      "Epoch: 31, Average Loss: 2.552, Val Metric: 100.0, nfeatures: [1, 1, 0, 1, 1], Overlap: 2\n",
      "Epoch: 32, Average Loss: 2.552, Val Metric: 100.0, nfeatures: [1, 1, 0, 1, 1], Overlap: 2\n",
      "Epoch: 33, Average Loss: 2.544, Val Metric: 100.0, nfeatures: [1, 1, 0, 1, 1], Overlap: 2\n",
      "Epoch: 34, Average Loss: 2.540, Val Metric: 100.0, nfeatures: [1, 1, 0, 1, 1], Overlap: 2\n",
      "Epoch: 35, Average Loss: 2.535, Val Metric: 100.0, nfeatures: [1, 1, 0, 1, 1], Overlap: 2\n"
     ]
    }
   ],
   "source": [
    "train_data = NumpyDataset(X_train, y_train, classification=is_classification)\n",
    "val_data = NumpyDataset(X_val, y_val, classification=is_classification)\n",
    "model = TorchModel(compfs_config)\n",
    "model.train(train_data, val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9tiWX3lWGXcf",
    "outputId": "ce83a0d1-c833-4858-84ed-844db72df098"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Group Structure:\n",
      "Group Similarity: 1.000, True Positive Rate: 100.000%, False Discovery Rate: 0.000%\n",
      "Number of True Groups: 2, Number of Predicted Groups: 2\n",
      "\n",
      "\n",
      "Selected Features:\n",
      "Group: 1, Features: [1]\n",
      "Group: 2, Features: [0]\n"
     ]
    }
   ],
   "source": [
    "# Get group similarity and group structure.\n",
    "tpr, fdr = tpr_fdr(ground_truth_groups, model.get_groups())\n",
    "group_sim, ntrue, npredicted = gsim(ground_truth_groups, model.get_groups())\n",
    "\n",
    "print('\\n\\nGroup Structure:')\n",
    "print('Group Similarity: {:.3f}, True Positive Rate: {:.3f}%, False Discovery Rate: {:.3f}%'.format(group_sim, tpr, fdr))\n",
    "print('Number of True Groups: {}, Number of Predicted Groups: {}'.format(ntrue, npredicted))\n",
    "\n",
    "# Give selected features and save the groups.\n",
    "print('\\n\\nSelected Features:')\n",
    "learnt_groups = model.get_groups()\n",
    "for i in range(len(learnt_groups)):\n",
    "    print('Group: {}, Features: {}'.format(i+1, learnt_groups[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wsn30ZLfIIgF"
   },
   "source": [
    "We see that the model finds the features, usually separating features 0 and 1, occasionally grouping them together."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
